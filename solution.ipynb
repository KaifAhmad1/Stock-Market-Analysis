{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Choice Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.1: In the context of NLP, what does tokenization involve?\\nAnswer: c) Splitting the word into words or subwords \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.1: In the context of NLP, what does tokenization involve?\n",
    "Answer: c) Splitting the word into words or subwords \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.2: What is the main advantage of Convolutional Neural Networks (CNNs) over Fully\\nConnected Neural Networks in image analysis tasks?\\nAnswer: \\nc) They can capture spatial information\\nd) They can process images of varying sizes\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.2: What is the main advantage of Convolutional Neural Networks (CNNs) over Fully\n",
    "Connected Neural Networks in image analysis tasks?\n",
    "Answer: \n",
    "c) They can capture spatial information\n",
    "d) They can process images of varying sizes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.3: What is \"Stemming\" in NLP?\\nAnswer: b) Process of reducing words to their root or base form\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.3: What is \"Stemming\" in NLP?\n",
    "Answer: b) Process of reducing words to their root or base form\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.4: What is \"Word Embedding\" in NLP?\\nAnswer: a) Encoding words or phrases as numerical vectors\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.4: What is \"Word Embedding\" in NLP?\n",
    "Answer: a) Encoding words or phrases as numerical vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.5: What is \"Latent Dirichlet Allocation (LDA)\" used for in NLP?\\nAnswer: d) Topic Modelling\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.5: What is \"Latent Dirichlet Allocation (LDA)\" used for in NLP?\n",
    "Answer: d) Topic Modelling\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1.6: What is a \"Stop Word\" in the context of NLP?\\nAnswer: b) A word that is to be ignored in text analysis\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1.6: What is a \"Stop Word\" in the context of NLP?\n",
    "Answer: b) A word that is to be ignored in text analysis\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Text Processing\n",
    "\n",
    "Using the Python library NLTK, write a function that accepts a string and performs the following:\n",
    "\n",
    "-  Tokenizes the string into words\n",
    "-  Tags parts of speech (POS) for each word\n",
    "-  Performs named entity recognition (NER)\n",
    "-  Counts the frequency of each named entity in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Necessary Packages: \n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources \n",
    "nltk.download('punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the NE and POS in the given sentences: \n",
    "def process_sentences(sentences):\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        named_entities = nltk.ne_chunk(pos_tags)\n",
    "        \n",
    "        named_entity_counter = Counter(entity[0] for entity in named_entities if hasattr(entity, 'label'))\n",
    "        \n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(\"Named Entities:\", named_entity_counter)\n",
    "        print(\"POS Tags:\", pos_tags)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Alice and Bob went to the park on a beautiful sunny day.\n",
      "Named Entities: Counter({('Bob', 'NNP'): 1})\n",
      "POS Tags: [('Alice', 'NNP'), ('and', 'CC'), ('Bob', 'NNP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('park', 'NN'), ('on', 'IN'), ('a', 'DT'), ('beautiful', 'JJ'), ('sunny', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
      "--------------------------------------------------\n",
      "Sentence: They met their friends Carol and David, and together, they enjoyed a picnic with delicious sandwiches, fruits, and refreshing drinks.\n",
      "Named Entities: Counter({('Carol', 'NNP'): 1, ('David', 'NNP'): 1})\n",
      "POS Tags: [('They', 'PRP'), ('met', 'VBD'), ('their', 'PRP$'), ('friends', 'NNS'), ('Carol', 'NNP'), ('and', 'CC'), ('David', 'NNP'), (',', ','), ('and', 'CC'), ('together', 'RB'), (',', ','), ('they', 'PRP'), ('enjoyed', 'VBD'), ('a', 'DT'), ('picnic', 'NN'), ('with', 'IN'), ('delicious', 'JJ'), ('sandwiches', 'NNS'), (',', ','), ('fruits', 'NNS'), (',', ','), ('and', 'CC'), ('refreshing', 'VBG'), ('drinks', 'NNS'), ('.', '.')]\n",
      "--------------------------------------------------\n",
      "Sentence: The Eiffel Tower in Paris is a popular tourist attraction.\n",
      "Named Entities: Counter({('Eiffel', 'NNP'): 1, ('Paris', 'NNP'): 1})\n",
      "POS Tags: [('The', 'DT'), ('Eiffel', 'NNP'), ('Tower', 'NNP'), ('in', 'IN'), ('Paris', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('popular', 'JJ'), ('tourist', 'NN'), ('attraction', 'NN'), ('.', '.')]\n",
      "--------------------------------------------------\n",
      "Sentence: I saw a movie starring Tom Hanks and Julia Roberts.\n",
      "Named Entities: Counter({('Tom', 'NNP'): 1, ('Julia', 'NNP'): 1})\n",
      "POS Tags: [('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('movie', 'NN'), ('starring', 'VBG'), ('Tom', 'NNP'), ('Hanks', 'NNP'), ('and', 'CC'), ('Julia', 'NNP'), ('Roberts', 'NNP'), ('.', '.')]\n",
      "--------------------------------------------------\n",
      "Sentence: The company Apple Inc. announced a new product yesterday.\n",
      "Named Entities: Counter({('Apple', 'NNP'): 1})\n",
      "POS Tags: [('The', 'DT'), ('company', 'NN'), ('Apple', 'NNP'), ('Inc.', 'NNP'), ('announced', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('product', 'NN'), ('yesterday', 'NN'), ('.', '.')]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input  with multiple sentences\n",
    "sentences = [\n",
    "    \"Alice and Bob went to the park on a beautiful sunny day.\",\n",
    "    \"They met their friends Carol and David, and together, they enjoyed a picnic with delicious sandwiches, fruits, and refreshing drinks.\",\n",
    "    \"The Eiffel Tower in Paris is a popular tourist attraction.\",\n",
    "    \"I saw a movie starring Tom Hanks and Julia Roberts.\",\n",
    "    \"The company Apple Inc. announced a new product yesterday.\"\n",
    "]\n",
    "process_sentences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: PDF Processing and Table Extraction:\n",
    "\n",
    "Using Python and any library of your choice (such as Tabula, PDFplumber, etc.), write a\n",
    "function that accepts a path to a PDF file and:\n",
    "\n",
    "-  Extracts all the text from the PDF\n",
    "-  Identifies and extracts tables from the document\n",
    "-  Identifies and extracts all headings in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Packages: \n",
    "import pdfplumber\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pdfplumber.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Text and extracting information from the given PDF \n",
    "def pdf_processing(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            # Extract text from the PDF\n",
    "            full_text = \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "\n",
    "            # Extract tables from the PDF\n",
    "            tables = []\n",
    "            for page in pdf.pages:\n",
    "                tables.extend(page.extract_tables())\n",
    "\n",
    "            # Extract headings from the PDF\n",
    "            headings = [element[\"text\"] for page in pdf.pages for element in page.extract_words() if element.get(\"bold\", False) and element.get(\"size\", 0) > 10]\n",
    "\n",
    "        return {\n",
    "            \"text\": full_text,\n",
    "            \"tables\": tables,\n",
    "            \"headings\": headings\n",
    "        }\n",
    "    except PDFSyntaxError as e:\n",
    "        return {\"error\": f\"PDF Syntax Error: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input PDF:\n",
    "pdf_file_path = \"C:\\\\Users\\\\Mohd Kaif\\\\Downloads\\\\BART-research-paper.pdf\"\n",
    "result = pdf_processing(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural\n",
      "Language Generation, Translation, and Comprehension\n",
      "MikeLewis*,YinhanLiu*,NamanGoyal*,MarjanGhazvininejad,\n",
      "AbdelrahmanMohamed,OmerLevy,VesStoyanov,LukeZettlemoyer\n",
      "FacebookAI\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n",
      "Abstract maskedtokensarepredicted(Yangetal.,2019),andthe\n",
      "available context for replacing masked tokens (Dong\n",
      "We present BART, a denoising autoencoder et al., 2019). However, these methods typically focus\n",
      "for pretraining sequence-to-sequence models. on particular types of end tasks (e.g. span prediction,\n",
      "BARTistrainedby(1)corruptingtextwithan generation,etc.),limitingtheirapplicability.\n",
      "arbitrary noising function, and (2) learning a\n",
      "In this paper, we present BART, which pre-trains\n",
      "model to reconstruct the original text. It uses\n",
      "amodelcombiningBidirectionalandAuto-Regressive\n",
      "a standard Tranformer-based neural machine\n",
      "Transformers. BART is a denoising autoencoder built\n",
      "translationarchitecturewhich,despiteitssim-\n",
      "with a sequence-to-sequence model that is applicable\n",
      "plicity,canbeseenasgeneralizingBERT(due\n",
      "to a very wide range of end tasks. Pretraining has\n",
      "to the bidirectional encoder), GPT (with the twostages(1)textiscorruptedwithanarbitrarynois-\n",
      "left-to-rightdecoder),andmanyothermorere-\n",
      "ingfunction, and(2)asequence-to-sequencemodelis\n",
      "centpretrainingschemes. Weevaluateanum-\n",
      "learned to reconstruct the original text. BART uses a\n",
      "berofnoisingapproaches,findingthebestper-\n",
      "standard Tranformer-based neural machine translation\n",
      "formance by both randomly shuffling the or-\n",
      "architecturewhich,despiteitssimplicity,canbeseenas\n",
      "deroftheoriginalsentencesandusinganovel\n",
      "generalizing BERT (due to the bidirectional encoder),\n",
      "in-filling scheme, where spans of text are re-\n",
      "GPT (with the left-to-right decoder), and many other\n",
      "placed with a single mask token. BART is\n",
      "morerecentpretrainingschemes(seeFigure1).\n",
      "particularly effective when fine tuned for text\n",
      "Akeyadvantageofthissetupisthenoisingflexibil-\n",
      "generation but also works well for compre-\n",
      "ity;arbitrarytransformationscanbeappliedtotheorig-\n",
      "hension tasks. It matches the performance of\n",
      "inal text, including changing its length. We evaluate\n",
      "RoBERTawithcomparabletrainingresources\n",
      "a number of noising approaches, finding the best per-\n",
      "on GLUE and SQuAD, achieves new state-\n",
      "formance by both randomly shuffling the order of the\n",
      "of-the-art results on a range of abstractive di-\n",
      "originalsentencesandusinganovelin-fillingscheme,\n",
      "alogue, question answering, and summariza-\n",
      "where arbitrary length spans of text (including zero\n",
      "tion tasks, with gains of up to 6 ROUGE.\n",
      "length)arereplacedwithasinglemasktoken. Thisap-\n",
      "BARTalsoprovidesa1.1BLEUincreaseover\n",
      "proachgeneralizestheoriginalwordmaskingandnext\n",
      "aback-translationsystemformachinetransla-\n",
      "sentencepredictionobjectivesinBERTbyforcingthe\n",
      "tion,withonlytargetlanguagepretraining.We\n",
      "modeltoreasonmoreaboutoverallsentencelengthand\n",
      "alsoreportablationexperimentsthatreplicate\n",
      "makelongerrangetransformationstotheinput.\n",
      "other pretraining schemes within the BART\n",
      "BART is particularly effective when fine tuned for\n",
      "framework, to better measure which factors\n",
      "text generation but also works well for comprehen-\n",
      "mostinfluenceend-taskperformance.\n",
      "sion tasks. It matches the performance of RoBERTa\n",
      "(Liu et al., 2019) with comparable training resources\n",
      "1 Introduction\n",
      "onGLUE(Wangetal.,2018)andSQuAD(Rajpurkar\n",
      "Self-supervised methods have achieved remarkable et al., 2016), and achieves new state-of-the-art results\n",
      "successinawiderangeofNLPtasks (Mikolovetal., on a range of abstractive dialogue, question answer-\n",
      "2013; Peters et al., 2018; Devlin et al., 2019; Joshi ing, and summarization tasks. For example, it im-\n",
      "et al., 2019; Yang et al., 2019; Liu et al., 2019). provesperformanceby6ROUGEoverpreviouswork\n",
      "Themostsuccessfulapproacheshavebeenvariantsof onXSum(Narayanetal.,2018).\n",
      "maskedlanguagemodels,whicharedenoisingautoen- BARTalsoopensupnewwaysofthinkingaboutfine\n",
      "codersthataretrainedtoreconstructtextwherearan- tuning. Wepresentanewschemeformachinetransla-\n",
      "domsubsetofthewordshasbeenmaskedout. Recent tion where a BART model is stacked above a few ad-\n",
      "workhasshowngainsbyimprovingthedistributionof ditional transformer layers. These layers are trained\n",
      "masked tokens (Joshi et al., 2019), the order in which to essentially translate the foreign language to noised\n",
      "9102\n",
      "tcO\n",
      "92\n",
      "]LC.sc[\n",
      "1v16431.0191:viXra\n",
      "B D A B C D E\n",
      "Bidirectional Autoregressive\n",
      "Encoder Decoder\n",
      "A _ C _ E <s> A B C D\n",
      "(a)BERT:Randomtokensarereplacedwithmasks,and (b)GPT:Tokensarepredictedauto-regressively,meaning\n",
      "thedocumentisencodedbidirectionally. Missingtokens GPTcanbeusedforgeneration.Howeverwordscanonly\n",
      "are predicted independently, so BERT cannot easily be conditiononleftwardcontext,soitcannotlearnbidirec-\n",
      "usedforgeneration. tionalinteractions.\n",
      "A B C D E\n",
      "Bidirectional Autoregressive\n",
      "Encoder Decoder\n",
      "A _ B _ E <s> A B C D\n",
      "(c)BART:Inputstotheencoderneednotbealignedwithdecoderoutputs, allowingarbitarynoisetransformations. Here, a\n",
      "documenthasbeencorruptedbyreplacingspansoftextwithmasksymbols. Thecorrupteddocument(left)isencodedwith\n",
      "a bidirectionalmodel, and then thelikelihood of theoriginal document (right)is calculated withan autoregressive decoder.\n",
      "Forfine-tuning,anuncorrupteddocumentisinputtoboththeencoderanddecoder,andweuserepresentationsfromthefinal\n",
      "hiddenstateofthedecoder.\n",
      "Figure1: AschematiccomparisonofBARTwithBERT(Devlinetal.,2019)andGPT(Radfordetal.,2018).\n",
      "English, by propagation through BART, thereby us- coder, and for our large model we use 12 layers in\n",
      "ingBARTasapre-trainedtarget-sidelanguagemodel. each. Thearchitectureiscloselyrelatedtothatusedin\n",
      "This approach improves performance over a strong BERT,withthefollowingdifferences:(1)eachlayerof\n",
      "back-translation MT baseline by 1.1 BLEU on the thedecoderadditionallyperformscross-attentionover\n",
      "WMTRomanian-Englishbenchmark. the final hidden layer of the encoder (as in the trans-\n",
      "To better understand these effects, we also report former sequence-to-sequence model); and (2) BERT\n",
      "an ablation analysis that replicates other recently pro- uses an additional feed-forward network before word-\n",
      "posedtrainingobjectives. Thisstudyallowsustocare- prediction,whichBARTdoesnot. Intotal,BARTcon-\n",
      "fully control for a number of factors, including data tains roughly 10% more parameters than the equiva-\n",
      "and optimization parameters, which have been shown lentlysizedBERTmodel.\n",
      "to be as important for overall performance as the se-\n",
      "2.2 Pre-trainingBART\n",
      "lectionoftrainingobjectives(Liuetal.,2019). Wefind\n",
      "thatBARTexhibitsthemostconsistentlystrongperfor- BARTistrainedbycorruptingdocumentsandthenop-\n",
      "manceacrossthefullrangeoftasksweconsider. timizing a reconstruction loss—the cross-entropy be-\n",
      "tweenthedecoder’soutputandtheoriginaldocument.\n",
      "2 Model Unlikeexistingdenoisingautoencoders,whicharetai-\n",
      "lored to specific noising schemes, BART allows us to\n",
      "BARTisadenoisingautoencoderthatmapsacorrupted applyanytypeofdocumentcorruption. Intheextreme\n",
      "documenttotheoriginaldocumentitwasderivedfrom. case, where all information about the source is lost,\n",
      "It is implemented as a sequence-to-sequence model BARTisequivalenttoalanguagemodel.\n",
      "with a bidirectional encoder over corrupted text and a Weexperimentwithseveralpreviouslyproposedand\n",
      "left-to-right autoregressive decoder. For pre-training, novel transformations, but we believe there is a sig-\n",
      "weoptimizethenegativeloglikelihoodoftheoriginal nificant potential for development of other new alter-\n",
      "document. natives. Thetransformationsweusedaresummarized\n",
      "below,andexamplesareshowninFigure2.\n",
      "2.1 Architecture\n",
      "Token Masking Following BERT (Devlin et al.,\n",
      "BART uses the standard sequence-to-sequence Trans-\n",
      "2019), random tokens are sampled and replaced with\n",
      "former architecture from (Vaswani et al., 2017), ex-\n",
      "[MASK]elements.\n",
      "cept, following GPT, that we modify ReLU activa-\n",
      "tionfunctionstoGeLUs(Hendrycks&Gimpel,2016) TokenDeletion Randomtokensaredeletedfromthe\n",
      "and initialise parameters from N(0,0.02). For our input. In contrast to token masking, the model must\n",
      "base model, we use 6 layers in the encoder and de- decidewhichpositionsaremissinginputs.\n",
      "A _C . _ E . D E . A B C . C . D E . A B\n",
      "Token Masking Sentence Permutation Document Rotation\n",
      "A . C . E . A B C . D E . A _ . D _ E .\n",
      "Token Deletion\n",
      "Text Infilling\n",
      "Figure2: Transformationsfornoisingtheinputthatweexperimentwith. Thesetransformationscanbecomposed.\n",
      "Text Infilling A number of text spans are sampled, input but manipulated, which is closely related to the\n",
      "with span lengths drawn from a Poisson distribution denoisingpre-trainingobjective. Here,theencoderin-\n",
      "(λ = 3). Eachspanisreplacedwithasingle[MASK] put is the input sequence, and the decoder generates\n",
      "token. 0-length spans correspond to the insertion of outputsautoregressively.\n",
      "[MASK] tokens. Text infilling is inspired by Span-\n",
      "BERT (Joshi et al., 2019), but SpanBERT samples 3.4 MachineTranslation\n",
      "spanlengthsfromadifferent(clampedgeometric)dis- WealsoexploreusingBARTtoimprovemachinetrans-\n",
      "tribution, and replaces each span with a sequence of lation decoders for translating into English. Previous\n",
      "[MASK]tokensofexactlythesamelength. Textinfill- workEdunov etal.(2019) has shownthatmodels can\n",
      "ingteachesthemodeltopredicthowmanytokensare beimprovedbyincorporatingpre-trainedencoders,but\n",
      "missingfromaspan. gains from using pre-trained language models in de-\n",
      "coders have been limited. We show that it is possible\n",
      "Sentence Permutation A document is divided into\n",
      "to use the entire BART model (both encoder and de-\n",
      "sentences based on full stops, and these sentences are\n",
      "coder)asasinglepretraineddecoderformachinetrans-\n",
      "shuffledinarandomorder.\n",
      "lation,byaddinganewsetofencoderparametersthat\n",
      "DocumentRotation Atokenischosenuniformlyat arelearnedfrombitext(seeFigure3b).\n",
      "random, and the document is rotated so that it begins Moreprecisely,wereplaceBART’sencoderembed-\n",
      "with that token. This task trains the model to identify ding layer with a new randomly initialized encoder.\n",
      "thestartofthedocument. Themodelistrainedend-to-end,whichtrainsthenew\n",
      "encodertomapforeignwordsintoaninputthatBART\n",
      "3 Fine-tuningBART can de-noise to English. The new encoder can use a\n",
      "separatevocabularyfromtheoriginalBARTmodel.\n",
      "TherepresentationsproducedbyBARTcanbeusedin\n",
      "We train the source encoder in two steps, in both\n",
      "severalwaysfordownstreamapplications.\n",
      "casesbackpropagatingthecross-entropylossfromthe\n",
      "outputoftheBARTmodel. Inthefirststep,wefreeze\n",
      "3.1 SequenceClassificationTasks\n",
      "most of BART parameters and only update the ran-\n",
      "Forsequenceclassificationtasks,thesameinputisfed\n",
      "domlyinitializedsourceencoder,theBARTpositional\n",
      "intotheencoderanddecoder,andthefinalhiddenstate\n",
      "embeddings,andtheself-attentioninputprojectionma-\n",
      "of the final decoder token is fed into new multi-class\n",
      "trixofBART’sencoderfirstlayer. Inthesecondstep,\n",
      "linear classifier. This approach is related to the CLS\n",
      "we train all model parameters for a small number of\n",
      "token in BERT; however we add the additional token\n",
      "iterations.\n",
      "to the end so that representation for the token in the\n",
      "decodercanattendtodecoderstatesfromthecomplete 4 ComparingPre-trainingObjectives\n",
      "input(Figure3a).\n",
      "BARTsupportsamuchwiderrangeofnoisingschemes\n",
      "3.2 TokenClassificationTasks\n",
      "duringpre-trainingthanpreviouswork. Wecomparea\n",
      "Fortokenclassificationtasks,suchasanswerendpoint rangeofoptionsusingbase-sizemodels(6encoderand\n",
      "classification for SQuAD, we feed the complete doc- 6decoderlayers,withahiddensizeof768),evaluated\n",
      "ument into the encoder and decoder, and use the top onarepresentativesubsetofthetaskswewillconsider\n",
      "hiddenstateofthedecoderasarepresentationforeach forthefulllargescaleexperimentsin§5.\n",
      "word. Thisrepresentationisusedtoclassifythetoken.\n",
      "4.1 ComparisonObjectives\n",
      "3.3 SequenceGenerationTasks\n",
      "While many pre-training objectives have been pro-\n",
      "BecauseBARThasanautoregressivedecoder,itcanbe posed, fair comparisons between these have been dif-\n",
      "directly fine tuned for sequence generation tasks such ficult to perform, at least in part due to differences in\n",
      "as abstractive question answering and summarization. training data, training resources, architectural differ-\n",
      "In both of these tasks, information is copied from the encesbetweenmodels,andfine-tuningprocedures. We\n",
      "label A B C D E\n",
      "Pre-trained Pre-trained\n",
      "Encoder Decoder\n",
      "Pre-trained Pre-trained\n",
      "Encoder Decoder <s> A B C D\n",
      "Randomly\n",
      "Initialized Encoder\n",
      "A B C D E <s> A B C D E\n",
      "α β γ δ ε\n",
      "(a) To use BART for classification problems, the same (b) Formachinetranslation,welearnasmalladditional\n",
      "inputisfedintotheencoderanddecoder,andtherepre- encoderthatreplacesthewordembeddingsinBART.The\n",
      "sentationfromthefinaloutputisused. newencodercanuseadisjointvocabulary.\n",
      "Figure3: FinetuningBARTforclassificationandtranslation.\n",
      "re-implement strong pre-training approaches recently We experiment with (1) treating the task as a stan-\n",
      "proposed for discriminative and generation tasks. We dardsequence-to-sequenceproblem, wherethesource\n",
      "aim,asmuchaspossible,tocontrolfordifferencesun- input to the encoder and the target is the decoder out-\n",
      "related to the pre-training objective. However, we do put, or (2) adding the source as prefix to the target in\n",
      "make minor changes to the learning rate and usage of the decoder, with a loss only on the target part of the\n",
      "layer normalisation in order to improve performance sequence. We find the former works better for BART\n",
      "(tuningtheseseparatelyforeachobjective). Forrefer- models,andthelatterforothermodels.\n",
      "ence,wecompareourimplementationswithpublished Tomostdirectlycompareourmodelsontheirability\n",
      "numbers from BERT, which was also trained for 1M tomodeltheirfine-tuningobjective(theloglikelihood\n",
      "steps on a combination of books and Wikipedia data. ofthehumantext),wereportperplexityinTable1.\n",
      "Wecomparethefollowingapproaches:\n",
      "4.2 Tasks\n",
      "Language Model Similarly to GPT (Radford et al.,\n",
      "SQuAD (Rajpurkaretal.,2016)aanextractiveques-\n",
      "2018), we train a left-to-right Transformer language\n",
      "tionansweringtaskonWikipediaparagraphs.Answers\n",
      "model. ThismodelisequivalenttotheBARTdecoder,\n",
      "aretextspansextractedfromagivendocumentcontext.\n",
      "withoutcross-attention.\n",
      "SimilartoBERT(Devlinetal.,2019),weuseconcate-\n",
      "PermutedLanguageModel BasedonXLNet(Yang nated question and context as input to the encoder of\n",
      "et al., 2019), we sample 1/6 of the tokens, and gener- BART,andadditionallypassthemtothedecoder. The\n",
      "atetheminarandomorderautoregressively. Forcon- model includes classifiers to predict the start and end\n",
      "sistency with other models, we do not implement the indicesofeachtoken.\n",
      "relativepositionalembeddingsorattentionacrossseg-\n",
      "MNLI (Williamsetal.,2017), abitextclassification\n",
      "mentsfromXLNet.\n",
      "task to predict whether one sentence entails another.\n",
      "MaskedLanguageModel FollowingBERT(Devlin The fine-tuned model concatenates the two sentences\n",
      "et al.,2019), we replace 15%of tokens with [MASK] withappendedanEOStoken,andpassesthemtoboth\n",
      "symbols, andtrainthemodeltoindependentlypredict the BART encoder and decoder. In contrast to BERT,\n",
      "theoriginaltokens. therepresentationoftheEOStokenisusedtoclassify\n",
      "thesentencesrelations.\n",
      "Multitask Masked Language Model As in UniLM\n",
      "(Dong et al., 2019), we train a Masked Language ELI5 (Fanetal.,2019),along-formabstractiveques-\n",
      "Model with additional self-attention masks. Self at- tionansweringdataset. Modelsgenerateanswerscon-\n",
      "tention masks are chosen randomly in with the follow ditioned on the concatenation of a question and sup-\n",
      "proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 un- portingdocuments.\n",
      "masked,and1/3withthefirst50%oftokensunmasked\n",
      "andaleft-to-rightmaskfortheremainder. XSum (Narayanetal.,2018),anewssummarization\n",
      "datasetwithhighlyabstractivesummaries.\n",
      "MaskedSeq-to-Seq InspiredbyMASS(Songetal.,\n",
      "2019), we mask a span containing 50% of tokens, ConvAI2 (Dinan et al., 2019), a dialogue response\n",
      "and train a sequence to sequence model to predict the generationtask,conditionedoncontextandapersona.\n",
      "maskedtokens.\n",
      "CNN/DM (Hermann et al., 2015), a news summa-\n",
      "For the Permuted LM, Masked LM and Multitask rization dataset. Summaries here are typically closely\n",
      "MaskedLM,weusetwo-streamattention(Yangetal., relatedtosourcesentences.\n",
      "2019) to efficiently compute likelihoods of the output\n",
      "4.3 Results\n",
      "part of the sequence (using a diagonal self-attention\n",
      "maskontheoutputtopredictwordsleft-to-right). ResultsareshowninTable1. Severaltrendsareclear:\n",
      "Model SQuAD1.1 MNLI ELI5 XSum ConvAI2 CNN/DM\n",
      "F1 Acc PPL PPL PPL PPL\n",
      "BERTBase(Devlinetal.,2019) 88.5 84.3 - - - -\n",
      "MaskedLanguageModel 90.0 83.5 24.77 7.87 12.59 7.06\n",
      "MaskedSeq2seq 87.0 82.1 23.40 6.80 11.43 6.19\n",
      "LanguageModel 76.7 80.1 21.40 7.00 11.51 6.56\n",
      "PermutedLanguageModel 89.1 83.7 24.03 7.69 12.23 6.96\n",
      "MultitaskMaskedLanguageModel 89.2 82.4 23.73 7.50 12.39 6.74\n",
      "BARTBase\n",
      "w/TokenMasking 90.4 84.1 25.05 7.08 11.73 6.10\n",
      "w/TokenDeletion 90.4 84.1 24.61 6.90 11.46 5.87\n",
      "w/TextInfilling 90.8 84.0 24.26 6.61 11.05 5.83\n",
      "w/DocumentRotation 77.2 75.3 53.69 17.14 19.87 10.59\n",
      "w/SentenceShuffling 85.4 81.5 41.87 10.93 16.67 7.89\n",
      "w/TextInfilling+SentenceShuffling 90.8 83.8 24.17 6.62 11.12 5.41\n",
      "Table1: Comparisonofpre-trainingobjectives. Allmodelsareofcomparablesizeandaretrainedfor1Msteps\n",
      "on a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data\n",
      "using the same code-base, and fine-tuned with the same procedures. Entries in the second block are inspired by\n",
      "pre-trainingobjectivesproposedinpreviouswork,buthavebeensimplifiedtofocusonevaluationobjectives(see\n",
      "§4.1). Performancevariesconsiderablyacrosstasks,buttheBARTmodelswithtextinfillingdemonstratethemost\n",
      "consistentlystrongperformance.\n",
      "Performanceofpre-trainingmethodsvariessignifi- Pure language models perform best on ELI5 The\n",
      "cantlyacrosstasks Theeffectivenessofpre-training ELI5 dataset is an outlier, with much higher perplex-\n",
      "methods is highly dependent on the task. For exam- ities than other tasks, and is the only generation task\n",
      "ple, a simple language model achieves the best ELI5 where other models outperform BART. A pure lan-\n",
      "performance,buttheworstSQUADresults. guage model performs best, suggesting that BART is\n",
      "less effective when the output is only loosely con-\n",
      "Token masking is crucial Pre-training objectives strainedbytheinput.\n",
      "based on rotating documents or permuting sentences\n",
      "perform poorly in isolation. The successful methods BARTachievesthemostconsistentlystrongperfor-\n",
      "either use token deletion or masking, or self-attention mance. With the exception of ELI5, BART models\n",
      "masks. Deletion appears to outperform masking on usingtext-infillingperformwellonalltasks.\n",
      "generationtasks.\n",
      "5 Large-scalePre-trainingExperiments\n",
      "Left-to-right pre-training improves generation\n",
      "Recentworkhasshownthatdownstreamperformance\n",
      "The Masked Language Model and the Permuted\n",
      "can dramatically improve when pre-training is scaled\n",
      "Language Model perform less well than others on\n",
      "tolargebatchsizes(Yangetal.,2019;Liuetal.,2019)\n",
      "generation, and are the only models we consider that\n",
      "and corpora. To test how well BART performs in this\n",
      "do not include left-to-right auto-regressive language\n",
      "regime, and to create a useful model for downstream\n",
      "modellingduringpre-training.\n",
      "tasks, we trained BART using the same scale as the\n",
      "RoBERTamodel.\n",
      "Bidirectional encoders are crucial for SQuAD As\n",
      "noted in previous work (Devlin et al., 2019), just\n",
      "5.1 ExperimentalSetup\n",
      "left-to-right decoder performs poorly on SQuAD, be-\n",
      "cause future context is crucial in classification deci- Wepre-trainalargemodelwith12layersineachofthe\n",
      "sions. However, BART achieves similar performance encoder and decoder, and a hidden size of 1024. Fol-\n",
      "withonlyhalfthenumberofbidirectionallayers. lowingRoBERTa(Liuetal.,2019),weuseabatchsize\n",
      "of 8000, and train the model for 500000 steps. Docu-\n",
      "Thepre-trainingobjectiveisnottheonlyimportant mentsaretokenizedwiththesamebyte-pairencoding\n",
      "factor OurPermutedLanguageModelperformsless asGPT-2(Radfordetal.,2019).Basedontheresultsin\n",
      "wellthanXLNet(Yangetal.,2019). Someofthisdif- Section §4, we use a combination of text infilling and\n",
      "ferenceislikelyduetonotincludingotherarchitectural sentencepermutation. Wemask30%oftokensineach\n",
      "improvements,suchasrelative-positionembeddingsor document, and permute all sentences. Although sen-\n",
      "segment-levelrecurrence. tencepermutationonlyshowssignificantadditivegains\n",
      "SQuAD1.1 SQuAD2.0 MNLI SST QQP QNLI STS-B RTE MRPC CoLA\n",
      "EM/F1 EM/F1 m/mm Acc Acc Acc Acc Acc Acc Mcc\n",
      "BERT 84.1/90.9 79.0/81.8 86.6/- 93.2 91.3 92.3 90.0 70.4 88.0 60.6\n",
      "UniLM -/- 80.5/83.4 87.0/85.9 94.5 - 92.7 - 70.9 - 61.1\n",
      "XLNet 89.0/94.5 86.1/88.8 89.8/- 95.6 91.8 93.9 91.8 83.8 89.2 63.6\n",
      "RoBERTa 88.9/94.6 86.5/89.4 90.2/90.2 96.4 92.2 94.7 92.4 86.6 90.9 68.0\n",
      "BART 88.8/94.6 86.1/89.2 89.9/90.1 96.6 92.5 94.9 91.2 87.0 90.4 62.8\n",
      "Table 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and\n",
      "XLNet,suggestingthatBART’suni-directionaldecoderlayersdonotreduceperformanceondiscriminativetasks.\n",
      "CNN/DailyMail XSum\n",
      "R1 R2 RL R1 R2 RL\n",
      "Lead-3 40.42 17.62 36.67 16.30 1.60 11.95\n",
      "PTGEN(Seeetal.,2017) 36.44 15.66 33.42 29.70 9.21 23.24\n",
      "PTGEN+COV(Seeetal.,2017) 39.53 17.28 36.38 28.10 8.02 21.72\n",
      "UniLM 43.33 20.21 40.51 - - -\n",
      "BERTSUMABS(Liu&Lapata,2019) 41.72 19.39 38.76 38.76 16.33 31.15\n",
      "BERTSUMEXTABS(Liu&Lapata,2019) 42.13 19.60 39.18 38.81 16.50 31.27\n",
      "BART 44.16 21.28 40.90 45.14 22.27 37.25\n",
      "Table3: Resultsontwostandardsummarizationdatasets. BARToutperformspreviousworkonsummarizationon\n",
      "twotasksandallmetrics,withgainsofroughly6pointsonthemoreabstractivedataset.\n",
      "on the CNN/DM summarization dataset, we hypothe- ConvAI2\n",
      "sisedthatlargerpre-trainedmodelsmaybebetterable ValidF1 ValidPPL\n",
      "tolearnfromthistask. Tohelpthemodelbetterfitthe\n",
      "Seq2Seq+Attention 16.02 35.07\n",
      "data,wedisableddropoutforthefinal10%oftraining\n",
      "BestSystem 19.09 17.51\n",
      "steps. We use the same pre-training data as Liu et al.\n",
      "BART 20.72 11.85\n",
      "(2019), consisting of 160Gb of news, books, stories,\n",
      "andwebtext.\n",
      "Table4: BARToutperformspreviousworkonconver-\n",
      "5.2 DiscriminativeTasks sational response generation. Perplexities are renor-\n",
      "malizedbasedonofficialtokenizerforConvAI2.\n",
      "Table2comparestheperformanceofBARTwithsev-\n",
      "eralrecentapproachesonthewell-studiedSQuADand\n",
      "GLUEtasks(Warstadtetal.,2018;Socheretal.,2013;\n",
      "Summarization To provide a comparison with the\n",
      "Dolan&Brockett,2005;Agirreetal.,2007;Williams\n",
      "state-of-the-art in summarization, we present results\n",
      "etal.,2018;Daganetal.,2006;Levesqueetal.,2011).\n",
      "on two summarization datasets, CNN/DailyMail and\n",
      "ThemostdirectlycomparablebaselineisRoBERTa,\n",
      "XSum,whichhavedistinctproperties.\n",
      "which was pre-trained with the same resources, but\n",
      "SummariesintheCNN/DailyMailtendtoresemble\n",
      "a different objective. Overall, BART performs simi-\n",
      "sourcesentences. Extractivemodelsdowellhere,and\n",
      "larly, with only small differences between the models\n",
      "even the baseline of the first-three source sentences is\n",
      "on most tasks. suggesting that BART’s improvements\n",
      "highly competitive. Nevertheless, BART outperforms\n",
      "ongenerationtasksdonotcomeattheexpenseofclas-\n",
      "allexistingwork.\n",
      "sificationperformance.\n",
      "Incontrast, XSumishighly abstractive, andextrac-\n",
      "tive models perform poorly. BART outperforms the\n",
      "5.3 GenerationTasks\n",
      "bestpreviouswork,whichleveragesBERT,byroughly\n",
      "Wealsoexperimentwithseveraltextgenerationtasks.\n",
      "6.0pointsonallROUGEmetrics—representingasig-\n",
      "BARTisfine-tunedasastandardsequence-to-sequence\n",
      "nificantadvanceinperformanceonthisproblem.Qual-\n",
      "model from the input to the output text. During fine-\n",
      "itatively,samplequalityishigh(see§6).\n",
      "tuning we use a label smoothed cross entropy loss\n",
      "(Pereyra et al., 2017), with the smoothing parameter Dialogue We evaluate dialogue response generation\n",
      "set to 0.1. During generation, we set beam size as 5, on CONVAI2 (Dinan et al., 2019), in which agents\n",
      "removeduplicatedtrigramsinbeamsearch,andtuned must generate responses conditioned on both the pre-\n",
      "themodelwithmin-len,max-len,lengthpenaltyonthe viouscontextandatextually-specifiedpersona. BART\n",
      "validationset(Fanetal.,2017). outperformspreviousworkontwoautomatedmetrics.\n",
      "ELI5 Table 7 shows example summaries generated by\n",
      "R1 R2 RL BART. Examples are taken from WikiNews articles\n",
      "publishedafterthecreationofthepre-trainingcorpus,\n",
      "BestExtractive 23.5 3.1 17.5\n",
      "toeliminatethepossibilityoftheeventsdescribedbe-\n",
      "LanguageModel 27.8 4.7 23.1\n",
      "ing present in the model’s training data. Following\n",
      "Seq2Seq 28.3 5.1 22.8\n",
      "Narayanetal.(2018), weremovethefirst sentenceof\n",
      "Seq2SeqMultitask 28.9 5.4 23.1\n",
      "the article prior to summarizing it, so there is no easy\n",
      "BART 30.6 6.2 24.3\n",
      "extractivesummaryofthedocument.\n",
      "Unsurprisingly,modeloutputisfluentandgrammat-\n",
      "Table 5: BART achieves state-of-the-art results on\n",
      "icalEnglish. However,modeloutputisalsohighlyab-\n",
      "the challenging ELI5 abstractive question answering\n",
      "stractive,withfewphrasescopiedfromtheinput. The\n",
      "dataset.ComparisonmodelsarefromFanetal.(2019).\n",
      "output is also generally factually accurate, and inte-\n",
      "grates supporting evidence from across the input doc-\n",
      "RO-EN ument with background knowledge (for example, cor-\n",
      "rectlycompletingnames,orinferringthatPG&Eoper-\n",
      "Baseline 36.80\n",
      "ates in California). In the first example, inferring that\n",
      "FixedBART 36.29\n",
      "fishareprotectingreefsfromglobalwarmingrequires\n",
      "TunedBART 37.96\n",
      "non-trivialinferencefromthetext. However,theclaim\n",
      "thattheworkwaspublishedinScienceisnotsupported\n",
      "Table 6: The performance (BLEU) of baseline and\n",
      "bythesource.\n",
      "BART on WMT’16 RO-EN augmented with back-\n",
      "ThesesamplesdemonstratethattheBARTpretrain-\n",
      "translation data. BART improves over a strong back-\n",
      "ing has learned a strong combination of natural lan-\n",
      "translation(BT)baselinebyusingmonolingualEnglish\n",
      "guageunderstandingandgeneration.\n",
      "pre-training.\n",
      "7 RelatedWork\n",
      "AbstractiveQA WeusetherecentlyproposedELI5\n",
      "Earlymethodsforpretrainingwerebasedonlanguage\n",
      "datasettotestthemodel’sabilitytogeneratelongfree-\n",
      "models. GPT (Radford et al., 2018) only models left-\n",
      "formanswers.WefindBARToutperformsthebestpre-\n",
      "ward context, which is problematic for some tasks.\n",
      "vious work by 1.2 ROUGE-L, but the dataset remains\n",
      "ELMo (Peters et al., 2018) concatenates left-only and\n",
      "achallenging,becauseanswersareonlyweaklyspeci-\n",
      "right-onlyrepresentations,butdoesnotpre-traininter-\n",
      "fiedbythequestion.\n",
      "actions between these features. Radford et al. (2019)\n",
      "demonstrated that very large language models can act\n",
      "5.4 Translation\n",
      "asunsupervisedmultitaskmodels.\n",
      "WealsoevaluatedperformanceonWMT16Romanian-\n",
      "BERT (Devlin et al., 2019) introduced masked lan-\n",
      "English, augmented with back-translation data\n",
      "guagemodelling,whichallowspre-trainingtolearnin-\n",
      "from Sennrich et al. (2016). We use a 6-layer\n",
      "teractions between left and right context words. Re-\n",
      "transformer source encoder to map Romanian into\n",
      "centworkhasshownthatverystrongperformancecan\n",
      "a representation that BART is able to de-noise into\n",
      "be achieved by training for longer (Liu et al., 2019),\n",
      "English, following the approach introduced in §3.4.\n",
      "by tying parameters across layers (Lan et al., 2019),\n",
      "Experiment results are presented in Table 6. We\n",
      "and by masking spans instead of words (Joshi et al.,\n",
      "compare our results against a baseline Transformer\n",
      "2019). Predictions are not made auto-regressively, re-\n",
      "architecture (Vaswani et al., 2017) with Transformer-\n",
      "ducingtheeffectivenessofBERTforgenerationtasks.\n",
      "large settings (the baseline row). We show the\n",
      "UniLM(Dongetal.,2019)fine-tunesBERTwithan\n",
      "performance of both steps of our model in the fixed\n",
      "ensembleofmasks,someofwhichallowonlyleftward\n",
      "BART and tuned BART rows. For each row we\n",
      "context. LikeBART,thisallowsUniLMtobeusedfor\n",
      "experimentontheoriginalWMT16Romanian-English\n",
      "bothgenerativeanddiscriminativetasks. Adifference\n",
      "augmented with back-translation data. We use a\n",
      "is that UniLM predictions are conditionally indepen-\n",
      "beam width of 5 and a length penalty of α = 1.\n",
      "dent, whereas BART’s are autoregressive. BART re-\n",
      "Preliminary results suggested that our approach was\n",
      "duces the mismatch between pre-training and genera-\n",
      "less effective without back-translation data, and prone\n",
      "tiontasks,becausethedecoderisalwaystrainedonun-\n",
      "to overfitting—future work should explore additional\n",
      "corruptedcontext.\n",
      "regularizationtechniques.\n",
      "MASS(Songetal.,2019)isperhapsthemostsimilar\n",
      "modeltoBART.Aninputsequencewhereacontiguous\n",
      "6 QualitativeAnalysis\n",
      "spanoftokensismaskedismappedtoasequencecon-\n",
      "BART shows large improvements on summarization sisting of the missing tokens. MASS is less effective\n",
      "metrics,ofupto6pointsoverthepriorstate-of-the-art. fordiscriminativetasks,becausedisjointsetsoftokens\n",
      "TounderstandBART’sperformancebeyondautomated arefedintotheencoderanddecoder.\n",
      "metrics,weanalyseitsgenerationsqualitatively. XL-Net (Yang et al., 2019) extends BERT by pre-\n",
      "SourceDocument(abbreviated) BARTSummary\n",
      "The researchers examined three types of coral in reefs off the Fisheries off the coast of Fiji are protect-\n",
      "coastofFiji... Theresearchersfoundwhenfishwereplentiful, ing coral reefs from the effects of global\n",
      "theywouldeatalgaeandseaweedoffthecorals,whichappeared warming,accordingtoastudyinthejour-\n",
      "toleavethemmoreresistanttothebacteriumVibriocoralliilyti- nalScience.\n",
      "cus,abacteriumassociatedwithbleaching.Theresearcherssug-\n",
      "gested the algae, like warming temperatures, might render the\n",
      "corals’ chemical defenses less effective, and the fish were pro-\n",
      "tectingthecoralbyremovingthealgae.\n",
      "Sacoolas,whohasimmunityasadiplomat’swife,wasinvolved BorisJohnsonhassaidhewillraisetheis-\n",
      "in a traffic collision ... Prime Minister Johnson was questioned sueofUSdiplomatAnneSacoolas’diplo-\n",
      "aboutthecasewhilespeakingtothepressatahospitalinWat- maticimmunitywiththeWhiteHouse.\n",
      "ford. He said, “I hope that Anne Sacoolas will come back ...\n",
      "if we can’t resolve it then of course I will be raising it myself\n",
      "personallywiththeWhiteHouse.”\n",
      "According to Syrian state media, government forces began de- Syrian government forces have entered\n",
      "ploying into previously SDF controlled territory yesterday. ... territory held by the US-backed Syrian\n",
      "On October 6, US President Donald Trump and Turkish Presi- Democratic Forces (SDF) in response to\n",
      "dent Recep Tayyip Erdoan spoke on the phone. Then both na- Turkey’sincursionintotheregion.\n",
      "tions issued statements speaking of an imminent incursion into\n",
      "northeast Syria ... . On Wednesday, Turkey began a military\n",
      "offensivewithairstrikesfollowedbyagroundinvasion.\n",
      "This is the first time anyone has been recorded to run a full Kenyan runner Eliud Kipchoge has run a\n",
      "marathon of 42.195 kilometers (approximately 26 miles) under marathoninlessthantwohours.\n",
      "this pursued landmark time. It was not, however, an officially\n",
      "sanctioned world record, as it was not an ”open race” of the\n",
      "IAAF.Histimewas1hour59minutes40.2seconds. Kipchoge\n",
      "raninVienna, Austria. Itwasaneventspecificallydesignedto\n",
      "helpKipchogebreakthetwohourbarrier.\n",
      "PG&Estateditscheduledtheblackoutsinresponsetoforecasts Power has been turned off to millions of\n",
      "forhighwindsamiddryconditions.Theaimistoreducetherisk customersinCaliforniaaspartofapower\n",
      "ofwildfires. Nearly800thousandcustomerswerescheduledto shutoffplan.\n",
      "beaffectedbytheshutoffswhichwereexpectedtolastthrough\n",
      "atleastmiddaytomorrow.\n",
      "Table7: ExamplesummariesfromtheXSum-tunedBARTmodelonWikiNewsarticles. Forclarity,onlyrelevant\n",
      "excerptsofthesourceareshown. Summariescombineinformationfromacrossthearticleandpriorknowledge.\n",
      "dictingmaskedtokensauto-regressivelyinapermuted 8 Conclusions\n",
      "order.Thisobjectiveallowspredictionstoconditionon\n",
      "We introduced BART, a pre-training approach that\n",
      "both left and right context. In contrast, the BART de-\n",
      "learns to map corrupted documents to the original.\n",
      "coderworksleft-to-rightduringpre-training,matching\n",
      "BART achieves similar performance to RoBERTa on\n",
      "thesettingduringgeneration.\n",
      "discriminativetasks,whileachievingnewstate-of-the-\n",
      "art results on a number of text generation tasks. Fu-\n",
      "ture work should explore new methods for corrupting\n",
      "Several papers have explored using pre-trained rep- documents for pre-training, perhaps tailoring them to\n",
      "resentations to improve machine translation. The specificendtasks.\n",
      "largest improvements have come from pre-training on\n",
      "both source and target languages (Song et al., 2019;\n",
      "Lample & Conneau, 2019), but this requires pre-\n",
      "training on all languages of interest. Other work has\n",
      "shownthatencoderscanbeimprovedusingpre-trained\n",
      "representations (Edunov et al., 2019), but gains in de-\n",
      "coders are more limited. We show how BART can be\n",
      "usedtoimprovemachinetranslationdecoders.\n",
      "References Karl Moritz Hermann, Tomas Kocisky, Edward\n",
      "Grefenstette,LasseEspeholt,WillKay,MustafaSu-\n",
      "Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-\n",
      "leyman, and Phil Blunsom. Teaching machines to\n",
      "towski (eds.). Proceedings of the Fourth Interna- readandcomprehend. InAdvancesinneuralinfor-\n",
      "tionalWorkshoponSemanticEvaluations(SemEval-\n",
      "mationprocessingsystems,pp.1693–1701,2015.\n",
      "2007). Association for Computational Linguistics,\n",
      "Prague,CzechRepublic,June2007. MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,\n",
      "Luke Zettlemoyer, and Omer Levy. Spanbert: Im-\n",
      "Ido Dagan, Oren Glickman, and Bernardo Magnini. proving pre-training by representing and predicting\n",
      "The PASCAL recognising textual entailment chal- spans. arXivpreprintarXiv:1907.10529,2019.\n",
      "lenge. In Machine learning challenges. evaluat-\n",
      "Guillaume Lample and Alexis Conneau. Cross-\n",
      "ing predictive uncertainty, visual object classifica-\n",
      "lingual language model pretraining. arXiv preprint\n",
      "tion, and recognising tectual entailment, pp. 177–\n",
      "arXiv:1901.07291,2019.\n",
      "190.Springer,2006.\n",
      "Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
      "Kristina Toutanova. BERT: Pre-training of deep\n",
      "cut. Albert: A lite bert for self-supervised learn-\n",
      "bidirectional transformers for language understand-\n",
      "ing of language representations. arXiv preprint\n",
      "ing. In Proceedings of the 2019 Conference of the\n",
      "arXiv:1909.11942,2019.\n",
      "NorthAmericanChapteroftheAssociationforCom-\n",
      "putationalLinguistics: HumanLanguageTechnolo- Hector J Levesque, Ernest Davis, and Leora Morgen-\n",
      "gies, Volume 1 (Long and Short Papers), pp. 4171– stern. The Winograd schema challenge. In AAAI\n",
      "4186,Minneapolis,Minnesota,June2019.Associa- SpringSymposium: LogicalFormalizationsofCom-\n",
      "tion for Computational Linguistics. doi: 10.18653/ monsenseReasoning,volume46,pp. 47,2011.\n",
      "v1/N19-1423. URL https://www.aclweb.\n",
      "org/anthology/N19-1423. Yang Liu and Mirella Lapata. Text summariza-\n",
      "tion with pretrained encoders. arXiv preprint\n",
      "Emily Dinan, Varvara Logacheva, Valentin Malykh, arXiv:1908.08345,2019.\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Lowe, et al. The second conversational in-\n",
      "Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\n",
      "telligence challenge (convai2). arXiv preprint\n",
      "A robustly optimized bert pretraining approach.\n",
      "arXiv:1902.00098,2019.\n",
      "arXivpreprintarXiv:1907.11692,2019.\n",
      "William B Dolan and Chris Brockett. Automatically Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\n",
      "constructing a corpus of sentential paraphrases. In Dean. Efficient estimation of word representations\n",
      "ProceedingsoftheInternationalWorkshoponPara- in vector space. arXiv preprint arXiv:1301.3781,\n",
      "phrasing,2005.\n",
      "2013.\n",
      "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- Shashi Narayan, Shay B Cohen, and Mirella Lapata.\n",
      "aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Don’tgivemethedetails, justthesummary! topic-\n",
      "andHsiao-WuenHon. Unifiedlanguagemodelpre- aware convolutional neural networks for extreme\n",
      "trainingfornaturallanguageunderstandingandgen- summarization. arXiv preprint arXiv:1808.08745,\n",
      "eration. arXivpreprintarXiv:1905.03197,2019. 2018.\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli. Gabriel Pereyra, George Tucker, Jan Chorowski,\n",
      "Pre-trained language model representations for lan- Łukasz Kaiser, and Geoffrey Hinton. Regularizing\n",
      "guagegeneration. InProceedingsofthe2019Con- neural networksby penalizing confidentoutput dis-\n",
      "ferenceoftheNorthAmericanChapteroftheAsso- tributions. arXivpreprintarXiv:1701.06548,2017.\n",
      "ciationforComputationalLinguistics: HumanLan-\n",
      "MatthewEPeters, MarkNeumann, MohitIyyer, Matt\n",
      "guage Technologies, Volume 1 (Long and Short Pa-\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "pers),2019.\n",
      "Zettlemoyer. Deepcontextualizedwordrepresenta-\n",
      "tions. arXivpreprintarXiv:1802.05365,2018.\n",
      "AngelaFan, DavidGrangier, andMichaelAuli. Con-\n",
      "trollable abstractive summarization. arXiv preprint Alec Radford, Karthik Narasimhan, Tim Salimans,\n",
      "arXiv:1711.05217,2017. and Ilya Sutskever. Improving language un-\n",
      "derstanding by generative pre-training. URL\n",
      "Angela Fan, Yacine Jernite, Ethan Perez, David https://s3-us-west-2. amazonaws. com/openai-\n",
      "Grangier, Jason Weston, and Michael Auli. Eli5: assets/researchcovers/languageunsupervised/language\n",
      "Long form question answering. arXiv preprint understandingpaper.pdf,2018.\n",
      "arXiv:1907.09190,2019.\n",
      "AlecRadford, JeffreyWu, RewonChild, DavidLuan,\n",
      "DanHendrycksandKevinGimpel. Gaussianerrorlin- DarioAmodei, andIlyaSutskever. Languagemod-\n",
      "earunits(gelus). arXivpreprintarXiv:1606.08415, els are unsupervised multitask learners. OpenAI\n",
      "2016. Blog,1(8),2019.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\n",
      "and Percy Liang. Squad: 100,000+ questions for\n",
      "machine comprehension of text. arXiv preprint\n",
      "arXiv:1606.05250,2016.\n",
      "Abigail See, Peter J Liu, and Christopher D\n",
      "Manning. Get to the point: Summarization\n",
      "with pointer-generator networks. arXiv preprint\n",
      "arXiv:1704.04368,2017.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "Edinburgh neural machine translation systems for\n",
      "WMT 16. In Proceedings of the First Conference\n",
      "onMachineTranslation: Volume2,SharedTaskPa-\n",
      "pers,2016.\n",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason\n",
      "Chuang, Christopher D Manning, Andrew Ng, and\n",
      "Christopher Potts. Recursive deep models for se-\n",
      "mantic compositionality over a sentiment treebank.\n",
      "InProceedingsofEMNLP,pp.1631–1642,2013.\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "YanLiu. Mass: Masked sequencetosequencepre-\n",
      "training for language generation. In International\n",
      "ConferenceonMachineLearning,2019.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. Attention is all you\n",
      "need. InAdvancesinneuralinformationprocessing\n",
      "systems,pp.5998–6008,2017.\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix\n",
      "Hill, Omer Levy, and Samuel R Bowman. Glue:\n",
      "A multi-task benchmark and analysis platform for\n",
      "natural language understanding. arXiv preprint\n",
      "arXiv:1804.07461,2018.\n",
      "Alex Warstadt, Amanpreet Singh, and Samuel R.\n",
      "Bowman. Neural network acceptability judgments.\n",
      "arXivpreprint1805.12471,2018.\n",
      "Adina Williams, Nikita Nangia, and Samuel R Bow-\n",
      "man. A broad-coverage challenge corpus for\n",
      "sentence understanding through inference. arXiv\n",
      "preprintarXiv:1704.05426,2017.\n",
      "Adina Williams, Nikita Nangia, and Samuel R. Bow-\n",
      "man. A broad-coverage challenge corpus for sen-\n",
      "tenceunderstandingthroughinference. InProceed-\n",
      "ingsofNAACL-HLT,2018.\n",
      "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime\n",
      "Carbonell, Ruslan Salakhutdinov, and Quoc V\n",
      "Le. Xlnet: Generalized autoregressive pretrain-\n",
      "ing for language understanding. arXiv preprint\n",
      "arXiv:1906.08237,2019.\n",
      "\n",
      "Extracted Tables:\n",
      "Table 1:\n",
      "[['A B C D E\\nAutoregressive\\nDecoder\\n<s> A B C D', 'A B C D E'], ['<s> A B C D', None]]\n",
      "\n",
      "Table 2:\n",
      "[['', '', '', '']]\n",
      "\n",
      "Table 3:\n",
      "[['A B C D E\\nBidirectional Autoregressive\\nEncoder Decoder\\nA _ B _ E <s> A B C D', 'A B C D E'], ['A _ B _ E', None]]\n",
      "\n",
      "Table 4:\n",
      "[['', '', '', '']]\n",
      "\n",
      "Table 5:\n",
      "[['label\\nPre-trained Pre-trained\\nEncoder Decoder\\nA B C D E <s> A B C D E', 'label']]\n",
      "\n",
      "Table 6:\n",
      "[['A B C D E\\nPre-trained Pre-trained\\nEncoder Decoder\\n<s> A B C D\\nRandomly\\nInitialized Encoder\\nα β γ δ ε', 'A B C D E']]\n",
      "\n",
      "Table 7:\n",
      "[['', '', '', '']]\n",
      "\n",
      "Table 8:\n",
      "[['α β γ δ ε', '']]\n",
      "\n",
      "\n",
      "Extracted Headings:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the results or error message\n",
    "if \"error\" in result:\n",
    "    print(result[\"error\"])\n",
    "else:\n",
    "    print(\"Extracted Text:\")\n",
    "    print(result[\"text\"])\n",
    "    print(\"\\nExtracted Tables:\")\n",
    "    for i, table in enumerate(result[\"tables\"]):\n",
    "        print(f\"Table {i + 1}:\")\n",
    "        print(table)\n",
    "        print()\n",
    "    print(\"\\nExtracted Headings:\")\n",
    "    print(result[\"headings\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
